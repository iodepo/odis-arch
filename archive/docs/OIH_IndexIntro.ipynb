{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Concepts \n",
    "\n",
    "## Review\n",
    "\n",
    "Before we starting pulling data lets revist the publishing overview \n",
    "presented in [Publisher](../../publishing/publishing.md) section.\n",
    "\n",
    "![overview](../../publishing/images/example1Flow.png)\n",
    "\n",
    "Briefly, a sitemap is made available that points the resources we will \n",
    "be indexing.  This can optionally be in a robots.txt file as well.  \n",
    "This sitemap will provide a URL for each resource we will be indexing.  \n",
    "It is fine if it providers more too, those resources simply wont express \n",
    "any JSON-LD content.  You can also have multiple sitemaps we ones specificly\n",
    "focused on the resources to index.  \n",
    "\n",
    "Each URL or page represented in the index are then accessed and parsed for the \n",
    "JSON-LD content.  \n",
    "\n",
    "## Command Line Tooling\n",
    "\n",
    "We can start exploring the indexing of data on the command line.  Here we will use the curl command which should \n",
    "be installed on all Mac OS X and Linux systems and should be found on the Linux Subsystem for Windows. \n",
    "\n",
    "This will give us a low level feel for what is going on.  \n",
    "\n",
    "We will start by exploring a sitemap.  \n",
    "\n",
    "```bash\n",
    "curl -s https://samples.earth/sitemap0.xml\n",
    "```\n",
    "\n",
    "We can parse out the URLs from the sitemap with the use of the UNIX grep command\n",
    "\n",
    "```bash\n",
    "curl -s https://samples.earth/sitemap0.xml |   grep -oP '<loc>\\K[^<]*'\n",
    "```\n",
    "\n",
    "In do this we see that out sitemap is really just a feed of URLs.  The sitemap provides us with the ability to add \n",
    "some extra information for our URLs.  It also providers a machine readable XML format we can work with.  There are many\n",
    "libraries for working with XML and several for working with the sitemap data model in XML as well. \n",
    "\n",
    "Now lets pull down the URL resource and parse out the JSON-LD we find in the ```<script>``` tag of type application/ld+json.\n",
    "\n",
    "Note, it is possible that there are many of these tags and also that this tag might be placed in by Javascript which means \n",
    "we would not see it here.  We will talk more about this as we explore.  For now we will just look for the first one and \n",
    "a static example, which we know this to be.  \n",
    "\n",
    "```bash\n",
    "curl -s  --header \"Accept: text/html\"   https://samples.earth/id/documents/c1pnht3h2h44frv6igfg | sed -n '/<script type=\\\"application\\/ld+json\\\">/,/<\\/script>/p'\n",
    "```\n",
    "\n",
    "Let's get rid of the ```<script> ``` tags and then parse the JSON-LD. \n",
    "\n",
    "```bash\n",
    "curl -s  --header \"Accept: text/html\"   https://samples.earth/id/documents/c1pnht3h2h44frv6igfg | sed -n '/<script type=\\\"application\\/ld+json\\\">/,/<\\/script>/p' | sed 's/<\\/script>//' | sed 's/<script type=\\\"application\\/ld+json\\\">//'\n",
    "```\n",
    "\n",
    "TODO WORK ON THIS\n",
    "At this point we could copy this JOSN-LD and visit something like the JSON-LD playground.   We could also \n",
    "visit the Structure Data Linter.   We can then play a bit with the JSON-LD there.\n",
    "\n",
    "\n",
    "Now, lets pass this through another app.  This is the jsonld.js app.  A Javascript app and library that can be found\n",
    "on GitHub at [digitalbazaar/jsonld.js](https://github.com/digitalbazaar/jsonld.js).   There are many similar libraries,\n",
    "so you can feel free to try out others.  \n",
    "\n",
    "\n",
    "```bash\n",
    "curl -s  --header \"Accept: text/html\"   https://samples.earth/id/documents/c1pnht3h2h44frv6igfg | sed -n '/<script type=\\\"application\\/ld+json\\\">/,/<\\/script>/p' | sed 's/<\\/script>//' | sed 's/<script type=\\\"application\\/ld+json\\\">//' | jsonld format -q\n",
    "```\n",
    "\n",
    "If all goes well we should see the following output.\n",
    "\n",
    "```\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/Dataset> .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/description> \"of data assignments.\" .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/distribution> _:b0 .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/maintainer> <https://samples.earth> .\n",
    "<https://samples.earth/id/documents/c1pnht3h2h44frv6igfg> <https://schema.org/name> \"Fake: technical and\" .\n",
    "<https://samples.earth> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/Organization> .\n",
    "<https://samples.earth> <https://schema.org/description> \"DEMO SITE:  fake data for testing\" .\n",
    "_:b0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/DataDownload> .\n",
    "_:b0 <https://schema.org/contentUrl> \"https://samples.earth/id/documents/c1pnht3h2h44frv6igfg.tsv\" .\n",
    "_:b0 <https://schema.org/encodingFormat> \"text/tab-separated-values\" .\n",
    "```\n",
    "\n",
    "So, this was just a simple set of command line calls to give us a feel for the process.  We have seen a sitemap, the URLs \n",
    "that make it up and pulled back those URLs and parsed the JSON-LD.  We then wen ahead and converted the JSON-LD into another\n",
    "RDF representation (triples) that make loading into a graph database easier.\n",
    "\n",
    "We could easily take these commands and roll them in a simple bash script.  This might not be a production level approach, \n",
    "but it's a good exercise and a good way to get started.  We will explore more advanced ways of doing this later.\n",
    "\n",
    "## Python\n",
    "\n",
    "Let's look at how this might be done in Python.  Python is a very popular language and has many solid libraries for working with JSON-LD. Again, we will use this more to get a feel for the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_ld_json(url: str) -> dict:\n",
    "    parser = \"html.parser\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, parser)\n",
    "    return json.loads(\"\".join(soup.find(\"script\", {\"type\":\"application/ld+json\"}).contents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "# from conceptnet5.uri import join_uri, split_uri\n",
    "API_ROOT = 'http://api.conceptnet.io'\n",
    "\n",
    "def short_name(value, max_length=40):\n",
    "    \"\"\"\n",
    "    Convert an RDF value (given as a dictionary) to a reasonable label.\n",
    "    \"\"\"\n",
    "    if value['type'] == 'blank node':\n",
    "        return '_'\n",
    "    elif value['type'] == 'IRI':    \n",
    "        url = value['value']\n",
    "        if '#' in url:\n",
    "            # Show just the fragment of URLs with a fragment\n",
    "            # (it's probably a property name)\n",
    "            return url.split('#')[-1]\n",
    "\n",
    "        # Give URLs relative to the root of our API\n",
    "        if url.startswith(API_ROOT):\n",
    "            short_url = url[len(API_ROOT):]\n",
    "            # If the URL is too long, hide it\n",
    "            if len(short_url) > max_length:\n",
    "                pieces = split_uri(short_url)\n",
    "                return join_uri(pieces[0], '...')\n",
    "            else:\n",
    "                return short_url\n",
    "        else:\n",
    "            return url.split('://')[-1]\n",
    "    else:\n",
    "        # Put literal values in quotes\n",
    "        text = value['value'].replace(':', '')\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length] + '...'\n",
    "        return '\"{}\"'.format(text)\n",
    "\n",
    "\n",
    "def show_graph(url, size=10):\n",
    "    \"\"\"\n",
    "    Show the graph structure of a ConceptNet API response.\n",
    "    \"\"\"\n",
    "    rdf = jsonld.normalize(url)['@default']\n",
    "    graph = graphviz.Digraph(\n",
    "        strict=False, graph_attr={'size': str(size), 'rankdir': 'LR'}\n",
    "    )\n",
    "    for edge in rdf:\n",
    "        subj = short_name(edge['subject'])\n",
    "        obj = short_name(edge['object'])\n",
    "        pred = short_name(edge['predicate'])\n",
    "        if subj and obj and pred:\n",
    "            # Apply different styles to the nodes based on whether they're\n",
    "            # literals, ConceptNet URLs, or other URLs\n",
    "            if obj.startswith('\"'):\n",
    "                # Literal values\n",
    "                graph.node(obj, penwidth='0')\n",
    "            elif obj.startswith('/'):\n",
    "                # ConceptNet nodes\n",
    "                graph.node(obj, style='filled', fillcolor=\"#ddeeff\")\n",
    "            else:\n",
    "                # Other URLs\n",
    "                graph.node(obj, color=\"#558855\")\n",
    "            graph.edge(subj, obj, label=pred)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse a search result\n",
    "\n",
    "Let's do a search at the OIH test web site and then see if we can work with some of the results there.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = get_ld_json(\"https://obis.org/dataset/46005357-02b8-4f17-b028-065bfc1cd384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@context\": {\n",
      "    \"@vocab\": \"https://schema.org/\"\n",
      "  },\n",
      "  \"@type\": \"Dataset\",\n",
      "  \"name\": \"Coral Reef Evaluation and Monitoring Project Dry Tortugas 1999\",\n",
      "  \"description\": \"The purpose of the Coral Reef Evaluation and Monitoring Project (CREMP) is to monitor the status and trends of selected reefs in the Florida Keys National Marine Sanctuary(FKNMS). CREMP assessments have been conducted annually at fixed sites since 1996 and data collected provides information on the temporal changes in benthic cover and diversity of stony corals and associated marine flora and fauna. The core field methods continue to be underwater videography and timed coral species inventories. Findings presented in this report include data from 109 stations at 37 sites sampled from 1996 through 2008 in the Florida Keys and 1999 through 2008 in the Dry Tortugas. The report describes the annual differences (between 2007 and 2008) in the percent cover of major benthic taxa (stony corals, octocorals, sponges, and macroalgae), mean coral species richness and the incidence of stony coral conditions. Additionally, it examines the long-term trends of the major benthic taxa, five coral complex, Montastraea cavernosa, Colpophyllia natans, Siderastrea siderea, and Porites astreoides) and the clionaid sponge, Cliona delitrix. It is one of the longest running coral reef monitoring projects in south Florida and has been extremely important in documenting the temporal changes that have occurred in recent years\",\n",
      "  \"url\": \"https://obis.org/dataset/46005357-02b8-4f17-b028-065bfc1cd384\",\n",
      "  \"sameAs\": [\n",
      "    \"https://www1.usgs.gov/obis-usa/ipt/resource?r=crempdrytortugas1999\"\n",
      "  ],\n",
      "  \"license\": \"To the extent possible under law, the publisher has waived all rights to these data and has dedicated them to the Public Domain (CC0 1.0)\",\n",
      "  \"citation\": null,\n",
      "  \"version\": \"2020-10-07T22:52:17.000Z\",\n",
      "  \"keywords\": [\n",
      "    \"Samplingevent\",\n",
      "    \"about\",\n",
      "    \"absence\",\n",
      "    \"accepted\",\n",
      "    \"accepted_aphia_database_identification_number\",\n",
      "    \"accepted_authorship_information\",\n",
      "    \"accepted_name_usage\",\n",
      "    \"acceptedNameAuthorship\",\n",
      "    \"acceptedNameUsage\",\n",
      "    \"acceptedNameUsageID\",\n",
      "    \"aphia\",\n",
      "    \"aphia_database_identification_number\",\n",
      "    \"area\",\n",
      "    \"array\",\n",
      "    \"array-data\",\n",
      "    \"associatedreferences\",\n",
      "    \"authorship\",\n",
      "    \"authorship_information\",\n",
      "    \"basis\",\n",
      "    \"basis of record\",\n",
      "    \"basisOfRecord\",\n",
      "    \"biogeographic\",\n",
      "    \"biology\",\n",
      "    \"class\",\n",
      "    \"commission\",\n",
      "    \"common\",\n",
      "    \"comprehensive\",\n",
      "    \"conservation\",\n",
      "    \"coral\",\n",
      "    \"coral monitoring\",\n",
      "    \"coverage\",\n",
      "    \"cremp\",\n",
      "    \"data\",\n",
      "    \"data_set_identification\",\n",
      "    \"data_set_name\",\n",
      "    \"database\",\n",
      "    \"dataset\",\n",
      "    \"datasetName\",\n",
      "    \"date\",\n",
      "    \"dry\",\n",
      "    \"dry tortugas\",\n",
      "    \"evaluation\",\n",
      "    \"event\",\n",
      "    \"eventdate\",\n",
      "    \"eventID\",\n",
      "    \"family\",\n",
      "    \"fish\",\n",
      "    \"florida\",\n",
      "    \"florida keys\",\n",
      "    \"fwc\",\n",
      "    \"fwc-fwri\",\n",
      "    \"fwri\",\n",
      "    \"genus\",\n",
      "    \"gulf of mexico\",\n",
      "    \"habitatid\",\n",
      "    \"hierarchy\",\n",
      "    \"identification\",\n",
      "    \"identifier\",\n",
      "    \"information\",\n",
      "    \"institute\",\n",
      "    \"kingdom\",\n",
      "    \"large\",\n",
      "    \"latitude\",\n",
      "    \"longitude\",\n",
      "    \"marine\",\n",
      "    \"meters\",\n",
      "    \"monitoring\",\n",
      "    \"name\",\n",
      "    \"number\",\n",
      "    \"obis\",\n",
      "    \"occurrence\",\n",
      "    \"occurrence_identification\",\n",
      "    \"occurrenceID\",\n",
      "    \"occurrenceStatus\",\n",
      "    \"ocean\",\n",
      "    \"order\",\n",
      "    \"organism\",\n",
      "    \"organism_per_sample_area\",\n",
      "    \"organismQuantity\",\n",
      "    \"organismQuantityType\",\n",
      "    \"per\",\n",
      "    \"percent\",\n",
      "    \"percent_coverage\",\n",
      "    \"phylum\",\n",
      "    \"presence\",\n",
      "    \"project\",\n",
      "    \"rank\",\n",
      "    \"record\",\n",
      "    \"recorded\",\n",
      "    \"recordedBy\",\n",
      "    \"reef\",\n",
      "    \"register\",\n",
      "    \"research\",\n",
      "    \"revisited\",\n",
      "    \"sample\",\n",
      "    \"samples\",\n",
      "    \"scientific\",\n",
      "    \"scientific_name\",\n",
      "    \"scientificname\",\n",
      "    \"scientificNameAuthorship\",\n",
      "    \"scientificnameid\",\n",
      "    \"set\",\n",
      "    \"sitecode\",\n",
      "    \"siteid\",\n",
      "    \"species\",\n",
      "    \"species_name\",\n",
      "    \"specificEpithet\",\n",
      "    \"square\",\n",
      "    \"statement\",\n",
      "    \"station\",\n",
      "    \"status\",\n",
      "    \"stewardship\",\n",
      "    \"subregionid\",\n",
      "    \"system\",\n",
      "    \"taxon\",\n",
      "    \"taxon_rank\",\n",
      "    \"taxon_status\",\n",
      "    \"taxonomic\",\n",
      "    \"taxonomic_status\",\n",
      "    \"taxonomicStatus\",\n",
      "    \"taxonomy\",\n",
      "    \"taxonRank\",\n",
      "    \"time\",\n",
      "    \"tortugas\",\n",
      "    \"unaccepted\",\n",
      "    \"usage\",\n",
      "    \"v2.3\",\n",
      "    \"value\",\n",
      "    \"vernacular\",\n",
      "    \"vernacular_name\",\n",
      "    \"vernacularName\",\n",
      "    \"wildlife\",\n",
      "    \"world\",\n",
      "    \"worms\",\n",
      "    \"year\"\n",
      "  ],\n",
      "  \"variableMeasured\": [],\n",
      "  \"includedInDataCatalog\": {\n",
      "    \"@id\": \"https://obis.org\",\n",
      "    \"@type\": \"DataCatalog\",\n",
      "    \"url\": \"https://obis.org\"\n",
      "  },\n",
      "  \"temporalCoverage\": \"1999/1999\",\n",
      "  \"distribution\": {\n",
      "    \"@type\": \"DataDownload\",\n",
      "    \"contentUrl\": \"https://www1.usgs.gov/obis-usa/ipt/archive.do?r=crempdrytortugas1999\",\n",
      "    \"encodingFormat\": \"application/zip\"\n",
      "  },\n",
      "  \"spatialCoverage\": {\n",
      "    \"@type\": \"Place\",\n",
      "    \"geo\": {\n",
      "      \"@type\": \"GeoShape\",\n",
      "      \"polygon\": \"-83.0022 24.6117,-83.0022 24.6993,-82.8702 24.6993,-82.8702 24.6117,-83.0022 24.6117\"\n",
      "    },\n",
      "    \"additionalProperty\": {\n",
      "      \"@type\": \"PropertyValue\",\n",
      "      \"propertyID\": \"http://dbpedia.org/resource/Spatial_reference_system\",\n",
      "      \"value\": \"http://www.w3.org/2003/01/geo/wgs84_pos#lat_long\"\n",
      "    }\n",
      "  },\n",
      "  \"provider\": [\n",
      "    {\n",
      "      \"@id\": \"https://oceanexpert.org/institution/5852\",\n",
      "      \"@type\": \"Organization\",\n",
      "      \"legalName\": \"Texas A&M University, College Station \\u2013 Department of Oceanography\",\n",
      "      \"name\": \"Texas A&M University, College Station \\u2013 Department of Oceanography\",\n",
      "      \"url\": \"https://oceanexpert.org/institution/5852\"\n",
      "    },\n",
      "    {\n",
      "      \"@id\": \"https://oceanexpert.org/institution/6238\",\n",
      "      \"@type\": \"Organization\",\n",
      "      \"legalName\": \"Florida Fish and Wildlife Conservation Commission, Fish and Wildlife Research Institute\",\n",
      "      \"name\": \"Florida Fish and Wildlife Conservation Commission, Fish and Wildlife Research Institute\",\n",
      "      \"url\": \"https://oceanexpert.org/institution/6238\"\n",
      "    },\n",
      "    {\n",
      "      \"@id\": \"https://oceanexpert.org/institution/12976\",\n",
      "      \"@type\": \"Organization\",\n",
      "      \"legalName\": \"U.S. Geological Survey HQ\",\n",
      "      \"name\": \"U.S. Geological Survey HQ\",\n",
      "      \"url\": \"https://oceanexpert.org/institution/12976\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_formatted_str = json.dumps(ld, indent=2)\n",
    "print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's big\n",
    "\n",
    "There is a lot here, especially in the keywords section.  That's great, this will give us a lot to work with the graph.  For this demo though it might be nice to parse it down a bit.  We can take advantage of a feature of JSON-LD called \"framing\".  Here will make a JSON-LD frame that allows us to parse out only those elements of the document we want to work with.  Framing is vey powerful, but we will work with a simple frame for now.  One like this:\n",
    "\n",
    "```json\n",
    " {\n",
    "  \"@context\": {\"@vocab\": \"https://schema.org/\"},\n",
    "  \"@explicit\": \"true\",\n",
    "   \"@type\":     \"Dataset\",\n",
    "   \"name\": \"\",\n",
    "   \"description\": \"\",\n",
    "   \"url\": \"\",\n",
    "   \"sameAs\": \"\",\n",
    "}\n",
    "```\n",
    "\n",
    "We will run this frame then look at the results and do a quick visualization using the defined function we placed at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@context\": {\n",
      "    \"@vocab\": \"https://schema.org/\"\n",
      "  },\n",
      "  \"@type\": \"Dataset\",\n",
      "  \"description\": \"The purpose of the Coral Reef Evaluation and Monitoring Project (CREMP) is to monitor the status and trends of selected reefs in the Florida Keys National Marine Sanctuary(FKNMS). CREMP assessments have been conducted annually at fixed sites since 1996 and data collected provides information on the temporal changes in benthic cover and diversity of stony corals and associated marine flora and fauna. The core field methods continue to be underwater videography and timed coral species inventories. Findings presented in this report include data from 109 stations at 37 sites sampled from 1996 through 2008 in the Florida Keys and 1999 through 2008 in the Dry Tortugas. The report describes the annual differences (between 2007 and 2008) in the percent cover of major benthic taxa (stony corals, octocorals, sponges, and macroalgae), mean coral species richness and the incidence of stony coral conditions. Additionally, it examines the long-term trends of the major benthic taxa, five coral complex, Montastraea cavernosa, Colpophyllia natans, Siderastrea siderea, and Porites astreoides) and the clionaid sponge, Cliona delitrix. It is one of the longest running coral reef monitoring projects in south Florida and has been extremely important in documenting the temporal changes that have occurred in recent years\",\n",
      "  \"name\": \"Coral Reef Evaluation and Monitoring Project Dry Tortugas 1999\",\n",
      "  \"sameAs\": \"https://www1.usgs.gov/obis-usa/ipt/resource?r=crempdrytortugas1999\",\n",
      "  \"url\": \"https://obis.org/dataset/46005357-02b8-4f17-b028-065bfc1cd384\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.44.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"698pt\" height=\"262pt\"\n viewBox=\"0.00 0.00 697.76 262.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 258)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-258 693.76,-258 693.76,4 -4,4\"/>\n<!-- schema.org/Dataset -->\n<g id=\"node1\" class=\"node\">\n<title>schema.org/Dataset</title>\n<ellipse fill=\"none\" stroke=\"#558855\" cx=\"471.38\" cy=\"-234\" rx=\"100.98\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"471.38\" y=\"-230.3\" font-family=\"Times-Roman\" font-size=\"14.00\">schema.org/Dataset</text>\n</g>\n<!-- _ -->\n<g id=\"node2\" class=\"node\">\n<title>_</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-122.3\" font-family=\"Times-Roman\" font-size=\"14.00\">_</text>\n</g>\n<!-- _&#45;&gt;schema.org/Dataset -->\n<g id=\"edge1\" class=\"edge\">\n<title>_&#45;&gt;schema.org/Dataset</title>\n<path fill=\"none\" stroke=\"black\" d=\"M34.9,-143.31C41.88,-158.34 54.18,-179.34 72,-190 164.81,-245.54 291.78,-249.86 377.21,-244.68\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"377.67,-248.15 387.42,-244 377.21,-241.17 377.67,-248.15\"/>\n<text text-anchor=\"middle\" x=\"153.5\" y=\"-242.8\" font-family=\"Times-Roman\" font-size=\"14.00\">type</text>\n</g>\n<!-- &quot;The purpose of the Coral Reef Evaluation...&quot; -->\n<g id=\"node3\" class=\"node\">\n<title>&quot;The purpose of the Coral Reef Evaluation...&quot;</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"0\" cx=\"471.38\" cy=\"-180\" rx=\"213.66\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"471.38\" y=\"-176.3\" font-family=\"Times-Roman\" font-size=\"14.00\">&quot;The purpose of the Coral Reef Evaluation...&quot;</text>\n</g>\n<!-- _&#45;&gt;&quot;The purpose of the Coral Reef Evaluation...&quot; -->\n<g id=\"edge2\" class=\"edge\">\n<title>_&#45;&gt;&quot;The purpose of the Coral Reef Evaluation...&quot;</title>\n<path fill=\"none\" stroke=\"black\" d=\"M49,-136.69C56.13,-139.88 64.26,-143.04 72,-145 131.06,-159.93 196.43,-168.74 256.85,-173.87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"256.64,-177.36 266.9,-174.69 257.22,-170.38 256.64,-177.36\"/>\n<text text-anchor=\"middle\" x=\"153.5\" y=\"-174.8\" font-family=\"Times-Roman\" font-size=\"14.00\">schema.org/description</text>\n</g>\n<!-- &quot;Coral Reef Evaluation and Monitoring Pro...&quot; -->\n<g id=\"node4\" class=\"node\">\n<title>&quot;Coral Reef Evaluation and Monitoring Pro...&quot;</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"0\" cx=\"471.38\" cy=\"-126\" rx=\"213.36\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"471.38\" y=\"-122.3\" font-family=\"Times-Roman\" font-size=\"14.00\">&quot;Coral Reef Evaluation and Monitoring Pro...&quot;</text>\n</g>\n<!-- _&#45;&gt;&quot;Coral Reef Evaluation and Monitoring Pro...&quot; -->\n<g id=\"edge3\" class=\"edge\">\n<title>_&#45;&gt;&quot;Coral Reef Evaluation and Monitoring Pro...&quot;</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.02,-126C92.97,-126 170.17,-126 247.63,-126\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"248.06,-129.5 258.06,-126 248.06,-122.5 248.06,-129.5\"/>\n<text text-anchor=\"middle\" x=\"153.5\" y=\"-129.8\" font-family=\"Times-Roman\" font-size=\"14.00\">schema.org/name</text>\n</g>\n<!-- &quot;https//www1.usgs.gov/obis&#45;usa/ipt/resour...&quot; -->\n<g id=\"node5\" class=\"node\">\n<title>&quot;https//www1.usgs.gov/obis&#45;usa/ipt/resour...&quot;</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"0\" cx=\"471.38\" cy=\"-72\" rx=\"215.56\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"471.38\" y=\"-68.3\" font-family=\"Times-Roman\" font-size=\"14.00\">&quot;https//www1.usgs.gov/obis&#45;usa/ipt/resour...&quot;</text>\n</g>\n<!-- _&#45;&gt;&quot;https//www1.usgs.gov/obis&#45;usa/ipt/resour...&quot; -->\n<g id=\"edge4\" class=\"edge\">\n<title>_&#45;&gt;&quot;https//www1.usgs.gov/obis&#45;usa/ipt/resour...&quot;</title>\n<path fill=\"none\" stroke=\"black\" d=\"M49,-115.31C56.13,-112.12 64.26,-108.96 72,-107 130.58,-92.19 195.37,-83.4 255.38,-78.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"255.68,-81.75 265.36,-77.43 255.11,-74.77 255.68,-81.75\"/>\n<text text-anchor=\"middle\" x=\"153.5\" y=\"-110.8\" font-family=\"Times-Roman\" font-size=\"14.00\">schema.org/sameAs</text>\n</g>\n<!-- &quot;https//obis.org/dataset/46005357&#45;02b8&#45;4f...&quot; -->\n<g id=\"node6\" class=\"node\">\n<title>&quot;https//obis.org/dataset/46005357&#45;02b8&#45;4f...&quot;</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"0\" cx=\"471.38\" cy=\"-18\" rx=\"218.26\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"471.38\" y=\"-14.3\" font-family=\"Times-Roman\" font-size=\"14.00\">&quot;https//obis.org/dataset/46005357&#45;02b8&#45;4f...&quot;</text>\n</g>\n<!-- _&#45;&gt;&quot;https//obis.org/dataset/46005357&#45;02b8&#45;4f...&quot; -->\n<g id=\"edge5\" class=\"edge\">\n<title>_&#45;&gt;&quot;https//obis.org/dataset/46005357&#45;02b8&#45;4f...&quot;</title>\n<path fill=\"none\" stroke=\"black\" d=\"M34.69,-108.72C41.59,-93.42 53.9,-71.89 72,-61 142.31,-18.7 232.4,-6.25 309.12,-5.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"309.4,-8.62 319.37,-5.04 309.34,-1.62 309.4,-8.62\"/>\n<text text-anchor=\"middle\" x=\"153.5\" y=\"-64.8\" font-family=\"Times-Roman\" font-size=\"14.00\">schema.org/url</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f1e096df1c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyld import jsonld\n",
    "import json\n",
    "\n",
    "frame = {\n",
    "  \"@context\": {\"@vocab\": \"https://schema.org/\"},\n",
    "  \"@explicit\": \"true\",\n",
    "   \"@type\":     \"Dataset\",\n",
    "   \"name\": \"\",\n",
    "   \"description\": \"\",\n",
    "   \"url\": \"\",\n",
    "   \"sameAs\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "framed = jsonld.frame(ld, frame)\n",
    "\n",
    "json_formatted_str = json.dumps(framed, indent=2)\n",
    "print(json_formatted_str)\n",
    "\n",
    "show_graph(framed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using these approaches you could explore data from other sources you know are publishing JSON-LD.  You could improve either of the bash script or Python code to loop on the resources and store the results in files or load them directly into a triples store / graph database.  \n",
    "\n",
    "As we continue, we will move on to the Gleaner package that is used by Ocean InfoHub and see some of the edge cases that can be addressed with it and how it fits into a more automated process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "884008db010a4da72f62f471ce341f5399c2c405d8eebc0270fe261741869d85"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
