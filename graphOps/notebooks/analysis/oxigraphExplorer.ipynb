{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ocean InfoHub Graph Explorer\n",
        "\n",
        "## Oxigraph explorer\n",
        "\n",
        "<a href=\"https://githubtocolab.com/gleanerio/archetype/blob/master/networks/commons/notebooks/networkViz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.png\" alt=\"Open in Colab\"/></a>\n",
        "\n",
        "At present the test with the available graphs represents just over 10 million triples."
      ],
      "metadata": {
        "id": "6hRr85vigOiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs, imports and definitions\n"
      ],
      "metadata": {
        "id": "hrxJQDSgxw1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2ZDVJx4n4gf7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q minio\n",
        "!pip install -q oxrdflib\n",
        "!pip install -q ipysigma\n",
        "!pip install -q kuzu\n",
        "!pip install -q pyoxigraph\n",
        "!pip install -q pygraphml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)  ## remove pandas future warning\n",
        "from minio import Minio\n",
        "from urllib.request import urlopen\n",
        "import kuzu\n",
        "from ipysigma import Sigma\n",
        "import os\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import networkx as nx\n",
        "from lxml import etree\n",
        "import io\n",
        "import pyoxigraph\n",
        "import re\n",
        "from rdflib import Graph\n",
        "\n",
        "from pygraphml import GraphMLParser\n",
        "from pygraphml import Graph as GraphML"
      ],
      "metadata": {
        "id": "sI2_aqTs-gfa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import mod\n",
        "\n",
        "# popper will convert nq to nt (via a simple hack), it will also convert http to https for schema.org prefixes\n",
        "def popper(input):\n",
        "    lines = input.splitlines()\n",
        "    modified_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        newline = line.replace(\"http://schema.org\", \"https://schema.org\")\n",
        "        segments = newline.split(' ')\n",
        "\n",
        "        if len(segments) > 3:\n",
        "            segments.pop()   # Remove the last two segment\n",
        "            segments.pop()\n",
        "            new_line = ' '.join(segments) + ' .'\n",
        "            modified_lines.append(new_line)\n",
        "\n",
        "\n",
        "    # print(len(modified_lines))\n",
        "    result_string = '\\n'.join(modified_lines)\n",
        "    # print(len(result_string))\n",
        "\n",
        "    return(result_string)\n",
        "\n",
        "# prefalign will convert http to https for schema.org prefixes\n",
        "def prefalign(input):\n",
        "    lines = input.splitlines()\n",
        "    modified_lines = []\n",
        "\n",
        "    regex = re.compile(r'[\\r\\n\\t]+')\n",
        "\n",
        "    for line in lines:\n",
        "        new_line = line.replace(\"http://schema.org\", \"https://schema.org\")\n",
        "        new_string = re.sub(regex, ' ', new_line)\n",
        "        modified_lines.append(new_string)\n",
        "\n",
        "\n",
        "    # print(len(modified_lines))\n",
        "    result_string = '\\n'.join(modified_lines)\n",
        "    # print(len(result_string))\n",
        "\n",
        "    return(result_string)\n",
        "\n",
        "\n",
        "def publicurls(client, bucket, prefix):\n",
        "    urls = []\n",
        "    objects = client.list_objects(bucket, prefix=prefix, recursive=True)\n",
        "    for obj in objects:\n",
        "        result = client.stat_object(bucket, obj.object_name)\n",
        "\n",
        "        if result.size > 0:  #  how to tell if an objet   obj.is_public  ?????\n",
        "            url = client.presigned_get_object(bucket, obj.object_name)\n",
        "            # print(f\"Public URL for object: {url}\")\n",
        "            urls.append(url)\n",
        "\n",
        "    return urls\n",
        "\n",
        "def download_file(url):\n",
        "    \"\"\"Downloads a remote file and handles potential errors.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)  # Stream for efficient memory usage\n",
        "        response.raise_for_status()  # Raise exception for error codes\n",
        "\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        local_filename = f\"./data/{filename.replace('.nq', '.nt')}\"  # Convert extension\n",
        "\n",
        "        with open(local_filename, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=1024):  # Download in chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Download failed for {url}: {e}\")"
      ],
      "metadata": {
        "id": "7_EIoLaGj4e8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Minio(\"ossapi.oceaninfohub.org:80\",  secure=False) # Create client with anonymous access.\n",
        "urls = publicurls(client, \"commons\", \"ODIS-KG-MAIN/18042024\")\n",
        "# for u in urls:\n",
        "#   print(u)"
      ],
      "metadata": {
        "id": "G5BEhyL-25rb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy files local for use\n",
        "\n",
        "You could load over the network directory into the triplestore.  For other tools like Kuzu this is not currently an option so I have the code here to pull down the files first, then load them."
      ],
      "metadata": {
        "id": "pMQttOx0Buob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory path\n",
        "directory_path = \"./data\"\n",
        "\n",
        "# Check if the directory exists and make it if it doesn't\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)"
      ],
      "metadata": {
        "id": "15Zi8NotSXtk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ThreadPoolExecutor() as executor:\n",
        "    for url in urls:\n",
        "        if \"_prov\" not in url:\n",
        "            executor.submit(download_file, url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMnNFt8Lp3nj",
        "outputId": "4eeb0754-9fbf-4287-cd50-d97c58f56887"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: inanodc_release.nq\n",
            "Downloaded: invemarvessels_release.nq\n",
            "Downloaded: invemarinstitutions_release.nq\n",
            "Downloaded: africaioc_release.nq\n",
            "Downloaded: invemartraining_release.nq\n",
            "Downloaded: marinetraining_release.nq\n",
            "Downloaded: invemarexperts_release.nq\n",
            "Downloaded: oceanscape_release.nq\n",
            "Downloaded: oceanexperts_release.nq\n",
            "Downloaded: medin_release.nq\n",
            "Downloaded: euroceanevents_release.nq\n",
            "Downloaded: wod_release.nq\n",
            "Downloaded: pedp_release.nq\n",
            "Downloaded: oceanexpert_release.nq\n",
            "Downloaded: rda_release.nq\n",
            "Downloaded: edmerp_release.nq\n",
            "Downloaded: edmo_release.nq\n",
            "Downloaded: euroceanexperts_release.nq\n",
            "Downloaded: obis_release.nq\n",
            "Downloaded: obps_release.nq\n",
            "Downloaded: cioos_release.nq\n",
            "Downloaded: pdh_release.nq\n",
            "Downloaded: aquadocs_release.nq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion section if needed\n",
        "\n",
        "In this section you can call code to either convert nq to nt and or align the various schema.org prefix values."
      ],
      "metadata": {
        "id": "4DEND3pfPNzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory path\n",
        "directory_path = \"./converted\"\n",
        "\n",
        "# Check if the directory exists and make it if it doesn't\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)"
      ],
      "metadata": {
        "id": "w3Plw_aPPsMJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"./data\"\n",
        "output_dir = \"./converted\"\n",
        "for f in os.listdir(dir):\n",
        "  fp = os.path.join(dir, f)\n",
        "\n",
        "  # prefalign\n",
        "  pa = prefalign(open(fp, 'r').read())\n",
        "  open(os.path.join(output_dir, f), 'w').write(pa)"
      ],
      "metadata": {
        "id": "pX1AzsZePZH7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLxkSRC9EyFc"
      },
      "source": [
        "## Load to Oxigraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NSWj9oKWHvyq"
      },
      "outputs": [],
      "source": [
        "# store = pyoxigraph.Store()  #    store = pyoxigraph.Store(path=\"./store\") # use path for disk store\n",
        "diskstore = pyoxigraph.Store(path=\"./store\")\n",
        "mime_type = \"application/n-quads\"   # application/n-triples or application/n-quads if you are loading those from data raw"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disk or Memory Store"
      ],
      "metadata": {
        "id": "Q9EGmhbbTvaI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wjyc-4xEE0AQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d792a9-f29f-40c4-8dc5-b314399f87de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: ./data/marinetraining_release.nt\n",
            "Loading: ./data/euroceanexperts_release.nt\n",
            "Loading: ./data/invemarinstitutions_release.nt\n",
            "Loading: ./data/obps_release.nt\n",
            "Loading: ./data/oceanexpert_release.nt\n",
            "Loading: ./data/invemarexperts_release.nt\n",
            "Loading: ./data/wod_release.nt\n",
            "Loading: ./data/invemartraining_release.nt\n",
            "Loading: ./data/edmo_release.nt\n",
            "Loading: ./data/cioos_release.nt\n",
            "Loading: ./data/euroceanevents_release.nt\n",
            "Loading: ./data/africaioc_release.nt\n",
            "Loading: ./data/pedp_release.nt\n",
            "Loading: ./data/inanodc_release.nt\n",
            "Loading: ./data/oceanexperts_release.nt\n",
            "Loading: ./data/aquadocs_release.nt\n",
            "Loading: ./data/oceanscape_release.nt\n",
            "Loading: ./data/rda_release.nt\n",
            "Loading: ./data/edmerp_release.nt\n",
            "Loading: ./data/pdh_release.nt\n",
            "Loading: ./data/obis_release.nt\n",
            "Loading: ./data/medin_release.nt\n",
            "Loading: ./data/invemarvessels_release.nt\n"
          ]
        }
      ],
      "source": [
        "dir = \"./data\"  # either of data or converted depending on what you did above\n",
        "\n",
        "# Use either or memory or disk store below.\n",
        "# Memory store is faster, but needs 24+ GB of memory\n",
        "# Disk store is slow, but runs in 16+ GB of memory (aquadocs spike?)\n",
        "\n",
        "# MEMORY STORE\n",
        "# for f in os.listdir(dir):\n",
        "#   fp = os.path.join(dir, f)\n",
        "#   print(f\"Loading: {fp}\")\n",
        "#   store.load(open(fp, 'rb'), mime_type, base_iri=None, to_graph=None)\n",
        "\n",
        "# DISK STORE\n",
        "for f in os.listdir(dir):\n",
        "  fp = os.path.join(dir, f)\n",
        "  print(f\"Loading: {fp}\")\n",
        "  diskstore.load(open(fp, 'rb'), mime_type, base_iri=None, to_graph=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SPARQL Query sections"
      ],
      "metadata": {
        "id": "LJ1TTRCWLcVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ic1_OjIUMej8"
      },
      "outputs": [],
      "source": [
        "qtest = \"\"\"\tPREFIX schema: <https://schema.org/>\n",
        "SELECT ?s\n",
        "WHERE {\n",
        "  graph ?g {\n",
        "     ?s ?p ?o  .\n",
        "     }\n",
        " } LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "q1 = list(diskstore.query(qtest))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qr)"
      ],
      "metadata": {
        "id": "SEIwBxpLiTWR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f04923d8-8949-47ed-e88c-3c8dbf1077d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'qr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fbd4e34b15ce>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'qr' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qtypetype = \"\"\"\tPREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
        "PREFIX schema: <https://schema.org/>\n",
        "\n",
        "SELECT DISTINCT ?source ?type ?target ?sType ?tType\n",
        "WHERE {\n",
        "    ?source a ?sType .\n",
        "    ?target a ?tType .\n",
        "    ?source ?type ?target .\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "q2 = list(storeobdisk.query(qtypetype))"
      ],
      "metadata": {
        "id": "ydsiKZlSlNe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0F5wXo8WlNM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}