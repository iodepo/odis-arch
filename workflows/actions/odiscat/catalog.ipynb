{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import pyoxigraph\n",
    "from pyld import jsonld\n",
    "import io\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def parse_sitemap(sitemap_url):\n",
    "    try:\n",
    "        # Fetch the sitemap\n",
    "        response = requests.get(sitemap_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the XML\n",
    "        root = ET.fromstring(response.content)\n",
    "\n",
    "        # Handle potential XML namespaces\n",
    "        namespace = {'ns': root.tag.split('}')[0].strip('{')} if '}' in root.tag else ''\n",
    "\n",
    "        # Extract URLs based on whether there's a namespace or not\n",
    "        if namespace:\n",
    "            urls = [url.find('ns:loc', namespace).text for url in root.findall('.//ns:url', namespace)]\n",
    "        else:\n",
    "            urls = [url.find('loc').text for url in root.findall('.//url')]\n",
    "\n",
    "        return urls\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching sitemap: {e}\")\n",
    "        return []\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_jsonld(url):\n",
    "    try:\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all script tags with type application/ld+json\n",
    "        jsonld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "\n",
    "        if jsonld_scripts:\n",
    "            # Return the content of the first JSON-LD script found\n",
    "            try:\n",
    "                # Attempt to parse and pretty-print the JSON\n",
    "                jsonld_data = json.loads(jsonld_scripts[0].string)\n",
    "                return json.dumps(jsonld_data, indent=2)\n",
    "            except json.JSONDecodeError:\n",
    "                # Return raw content if JSON parsing fails\n",
    "                return jsonld_scripts[0].string\n",
    "\n",
    "        return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None"
   ],
   "id": "c1d9eafb82b51923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# set up oxygraph\n",
    "store = pyoxigraph.Store()  # store = pyoxigraph.Store(path=\"./store\")\n",
    "mime_type = \"application/n-triples\"\n",
    "\n",
    "if len(sys.argv) != 2:\n",
    "    print(\"Usage: python script.py <sitemap_url>\")\n",
    "    sys.exit(1)\n",
    "\n",
    "sitemap_url = sys.argv[1]\n",
    "\n",
    "# Validate URL format\n",
    "try:\n",
    "    result = urlparse(sitemap_url)\n",
    "    if not all([result.scheme, result.netloc]):\n",
    "        raise ValueError(\"Invalid URL format\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Parse sitemap\n",
    "print(f\"Parsing sitemap: {sitemap_url}\")\n",
    "urls = parse_sitemap(sitemap_url)\n",
    "\n",
    "if not urls:\n",
    "    print(\"No URLs found in sitemap\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"Found {len(urls)} URLs in sitemap\")\n",
    "\n",
    "# Process each URL\n",
    "for url in urls:\n",
    "    print(f\"\\nChecking {url} for JSON-LD data...\")\n",
    "    jsonld_content = extract_jsonld(url)\n",
    "\n",
    "    if jsonld_content:\n",
    "        print(\"Found JSON-LD content:\")\n",
    "        print(jsonld_content)\n",
    "        normalized = jsonld.normalize(json.loads(jsonld_content), {'algorithm': 'URDNA2015', 'format': 'application/n-quads'})\n",
    "        x = store.load(io.StringIO(normalized), mime_type, base_iri=None, to_graph=None)\n",
    "        print(x)\n",
    "    else:\n",
    "        print(\"No JSON-LD content found\")"
   ],
   "id": "7835546e208ea4c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
